{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import h5py \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "sys.path.insert(0, \"../examples\")\n",
    "sys.path.insert(0, \"misato_dataset/components/\")\n",
    "from QMmodel import GNN_QM\n",
    "from MDmodel import GNN_MD\n",
    "from misato_dataset.components.transformQM import GNNTransformQM\n",
    "from misato_dataset.components.transformMD import GNNTransformMD\n",
    "from misato_dataset.processing.inference_QM import main\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation H5 file from a ligand pdbid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to run inference on a new structure from PDB. It is either possible to provide a already downloaded fileName or to just give the pdbid and it will be downloaded automatically. (If you run the script directly in the terminal just give the keywords in the promt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "  pdbid = \"vww\"\n",
    "  fileName = None\n",
    "  datasetOutName = 'inference_for_qm.hdf5'\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading vww.sdf\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of Ionization potential and Hardness by our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the created h5 file and store the elements and coordinates in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmh5_file = \"inference_for_qm.hdf5\"\n",
    "qm_H5File = h5py.File(qmh5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"x\", \"y\", \"z\", \"element\"]\n",
    "atoms = pd.DataFrame(columns = column_names)\n",
    "\n",
    "prop = qm_H5File[\"vww\"][\"atom_properties\"][\"atom_properties_values\"]\n",
    "atoms[\"x\"] = prop[:,0].astype(np.float32)\n",
    "atoms[\"y\"] = prop[:,1].astype(np.float32)\n",
    "atoms[\"z\"] = prop[:,2].astype(np.float32)\n",
    "        \n",
    "atoms[\"element\"] = np.array([element for element in qm_H5File['vww']['atom_properties']['atoms_names'][:]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = {\n",
    "    \"atoms\" : atoms,\n",
    "    \"labels\": 0,\n",
    "    \"bonds\": None, \n",
    "    \"id\": \"vww\"\n",
    "}\n",
    "\n",
    "transform = GNNTransformQM()\n",
    "data_item = transform(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run inference using cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNN_QM(\n",
       "  (lin0): Linear(in_features=25, out_features=64, bias=True)\n",
       "  (conv): NNConv(64, 64, aggr=mean, nn=Sequential(\n",
       "    (0): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=4096, bias=True)\n",
       "  ))\n",
       "  (gru): GRU(64, 64)\n",
       "  (set2set): Set2Set(64, 128)\n",
       "  (lin1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (lin2): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GNN_QM(data_item.num_features, 64)\n",
    "cpt = torch.load(\"../examples/logs/QM_latest/best_weights_rep0.pt\", map_location=torch.device('cpu'))[\"model_state_dict\"]\n",
    "model.load_state_dict(cpt)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with the model\n",
    "y_hat = model(data_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0480, -0.0375], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating H5 file for a protein-ligand complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the ligand case we download a pdb file, convert it to amber format and store it in an h5 file. For this step you need to have installed ambertools so you might have to switch the conda env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.processing.pdb_to_h5 import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdh5_file = \"inference_for_md.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "  pdbid = \"11GS\"\n",
    "  fileName = None\n",
    "  mapPath = \"data/processing/Maps/\"\n",
    "  mask = \"!@H=\" # no Hydrogens, see https://amberhub.chpc.utah.edu/atom-mask-selection-syntax/\n",
    "  datasetOutName = mdh5_file\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11GS/11GS.pdb was created. Please always use this file for inspection because the coordinates might get translated during amber file generation and thus might vary from the input pdb file.\n",
      "The following trajectory was created: pytraj.TrajectoryIterator, 1 frames: \n",
      "Size: 0.000146 (GB)\n",
      "<Topology: 6534 atoms, 416 residues, 2 mols, non-PBC>\n",
      "           \n",
      "molecule begin atom index [0, 1631, 3262] [1631, 1631]\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "  pdbid = \"11GS\"\n",
    "  fileName = \"11GS.pdb\"\n",
    "  mapPath = \"data/processing/Maps/\"\n",
    "  mask = \"\" # all atoms, see https://amberhub.chpc.utah.edu/atom-mask-selection-syntax/\n",
    "  datasetOutName = 'all_atoms_11GS.hdf5'\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11GS/11GS.pdb was created. Please always use this file for inspection because the coordinates might get translated during amber file generation and thus might vary from the input pdb file.\n",
      "The following trajectory was created: pytraj.TrajectoryIterator, 1 frames: \n",
      "Size: 0.000146 (GB)\n",
      "<Topology: 6534 atoms, 416 residues, 2 mols, non-PBC>\n",
      "           \n",
      "molecule begin atom index [0, 3267, 6534] [3267, 3267]\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of adaptability by our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to misato env if not running from container\n",
    "mdh5_file = \"inference_for_md.hdf5\"\n",
    "md_H5File = h5py.File(mdh5_file)\n",
    "\n",
    "column_names = [\"x\", \"y\", \"z\", \"element\"]\n",
    "atoms_protein = pd.DataFrame(columns = column_names)\n",
    "cutoff = md_H5File[\"11GS\"][\"molecules_begin_atom_index\"][:][-1] # cutoff defines protein atoms\n",
    "\n",
    "atoms_protein[\"x\"] = md_H5File[\"11GS\"][\"atoms_coordinates_ref\"][:][:cutoff, 0]\n",
    "atoms_protein[\"y\"] = md_H5File[\"11GS\"][\"atoms_coordinates_ref\"][:][:cutoff, 1]\n",
    "atoms_protein[\"z\"] = md_H5File[\"11GS\"][\"atoms_coordinates_ref\"][:][:cutoff, 2]\n",
    "\n",
    "atoms_protein[\"element\"] = md_H5File[\"11GS\"][\"atoms_element\"][:][:cutoff]  \n",
    "\n",
    "item = {}\n",
    "item[\"scores\"] = 0\n",
    "item[\"id\"] = \"11GS\"\n",
    "item[\"atoms_protein\"] = atoms_protein\n",
    "\n",
    "transform = GNNTransformMD()\n",
    "data_item = transform(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0, 1631, 3262])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " md_H5File[\"11GS\"][\"molecules_begin_atom_index\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNN_MD(\n",
       "  (conv1): GCNConv(11, 64)\n",
       "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): GCNConv(64, 128)\n",
       "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): GCNConv(128, 256)\n",
       "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): GCNConv(256, 256)\n",
       "  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): GCNConv(256, 512)\n",
       "  (bn5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "model = GNN_MD(data_item.num_features, 64)\n",
    "\n",
    "cpt = torch.load(\"../examples/logs/MD_latest/best_weights_rep0.pt\", map_location=torch.device('cpu'))[\"model_state_dict\"]\n",
    "\n",
    "model.load_state_dict(cpt)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3262])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data_item).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptability = model(data_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptability = adaptability.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step might be necessesary in case you have to change the kernel to ambertools env\n",
    "import pickle\n",
    "pickle.dump(adaptability.detach().numpy(), open('inference_adaptability.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Adaptability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96930208a3a4077a71ca55679b6a348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# switch to ambertools env if not running from container\n",
    "import nglview as nv\n",
    "import pytraj as pt\n",
    "import os\n",
    "import h5py\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptability = pickle.load(open('inference_adaptability.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_conversion(all_atom_file):\n",
    "    atoms_element= all_atom_file['atoms_element'][:]\n",
    "    atoms_coordinates_ref = all_atom_file['atoms_coordinates_ref'][:]\n",
    "    index_conversion = {}\n",
    "    noh_indices = np.where(atoms_element[:]!=1)[0] # change if not hydrogen\n",
    "    #h_indices = np.where(atoms_element[:]=1)[0]\n",
    "    equivalent_noh_index = 0\n",
    "    for all_atom_index in range(np.shape(atoms_coordinates_ref)[0]):\n",
    "        if all_atom_index in noh_indices:\n",
    "            index_conversion[equivalent_noh_index]=all_atom_index\n",
    "            equivalent_noh_index +=1\n",
    "    return index_conversion\n",
    "\n",
    "\n",
    "def show_ada_spheres(traj, ada_indices, residue_indices, prediction, color, radiusFactor):\n",
    "    for i in range(len(ada_indices)):\n",
    "        pred_mask = '@'+str(residue_indices[i]+1)\n",
    "        x,y,z = traj[pred_mask].xyz[:,:,:][0][0]\n",
    "        view.shape.add_sphere([x, y, z], color, prediction[ada_indices[i]]/radiusFactor)\n",
    "\n",
    "def add_opacity_to_spheres(num_spheres, opacity):\n",
    "    for i in range(num_spheres):\n",
    "        view.update_representation(component=view.n_components-i, opacity=opacity)\n",
    "        \n",
    "def convert_indices(indices, index_conversion):\n",
    "    values = []\n",
    "    for index in indices:\n",
    "        values.append(index_conversion[index])\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the h5 file with hydrogens and the h5 file with the hydrogens stripped (noh) after processing so that we assign the correct atom indices for the pdb file that we want to visualize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_inference = h5py.File('inference_for_md.hdf5', 'r')\n",
    "f_all_atom = h5py.File('all_atoms_11GS.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['atoms_coordinates_ref', 'atoms_element', 'atoms_number', 'atoms_residue', 'atoms_type', 'molecules_begin_atom_index']>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_inference['11GS'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"atoms_element\": shape (3262,), type \"<i8\">"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_inference[\"11GS\"]['atoms_element']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['atoms_coordinates_ref', 'atoms_element', 'atoms_number', 'atoms_residue', 'atoms_type', 'molecules_begin_atom_index']>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_all_atom[\"11GS\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_conversion = get_index_conversion(f_all_atom[\"11GS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_index_conversion= {value:key for key,value in index_conversion.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct = '11GS'\n",
    "traj = pt.load(struct+'/'+struct+'.pdb')\n",
    "view = nv.show_pytraj(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930516dbf9d84d95a8beb1e7308f652a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NGLWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residue_indices1 = list(traj.top.atom_indices(':1@C=,N=,O=,S='))\n",
    "residue_indices2 = list(traj.top.atom_indices(':327@C=,N=,O=,S='))\n",
    "\n",
    "residue_indices = residue_indices1+residue_indices2\n",
    "converted_indices = convert_indices(residue_indices, inverse_index_conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ada_spheres(traj, converted_indices, residue_indices, adaptability, (1,0,0), 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9b7a3c39ba4aa8b620c7c4913369b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', width='99%')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view.render_image(trim=True, factor=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view.download_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_opacity_to_spheres(view.n_components, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view.n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a740140079018068b3444b6d89694224b6cb3b34d8392a5487b58edeeeafee1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
